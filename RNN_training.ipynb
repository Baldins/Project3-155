{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout, Lambda\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text\n",
    "# f = open(os.getcwd()+'/data/shakespeare.txt')\n",
    "# text = f.readlines()\n",
    "\n",
    "# text = open('data/shakespeare.txt', 'r').read()\n",
    "# text = text.lower()\n",
    "\n",
    "f = open(os.getcwd()+'/data/shakespeare.txt')\n",
    "lines = f.readlines()\n",
    "all_words = []\n",
    "new_text = \"\"\n",
    "\n",
    "\n",
    "# important grammatical markers include ,:;?. spaces and \\n\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "#     newPunct = \"!#$%&()*'+,./:;<=>?@[\\]^_`{|}~1234567890-' '\"\n",
    "    newPun = \"#$%&()*+/<=>@[\\]^_`{|}~1234567890-\"\n",
    "    if len(line) > 0:\n",
    "        new_text += ''.join(c for c in line if c not in newPun) # changed from newPunct\n",
    "    if line.isdigit()==False:\n",
    "        all_words.append(line)\n",
    "\n",
    "# text with structure\n",
    "og_text = [x for x in all_words if x != []]\n",
    "\n",
    "# correct characters and dictionary\n",
    "chars = sorted(list(set(new_text)))\n",
    "chars = [x for x in chars if x not in newPun] # changed from newPunct\n",
    "\n",
    "# correct mappings\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "n_chars = len(new_text)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = len(chars)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 40\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = new_text[i:i + seq_length]\n",
    "\tseq_out = new_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "# print \"Total Patterns: \", n_patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.array([to_categorical(seq, num_classes=n_vocab) for seq in dataX])\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91310, 40, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 72915 x 40 \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm hard coding temp in the Lmbda line for ease of loading, temp = 0.2\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], num_chars)))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.add(Lambda(lambda x: x / 0.2))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91310/91310 [==============================] - 205s 2ms/step - loss: 2.9516\n",
      "Epoch 2/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 2.9082\n",
      "Epoch 3/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 2.6911\n",
      "Epoch 4/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 2.5115\n",
      "Epoch 5/100\n",
      "91310/91310 [==============================] - 200s 2ms/step - loss: 2.3946\n",
      "Epoch 6/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 2.3149\n",
      "Epoch 7/100\n",
      "91310/91310 [==============================] - 196s 2ms/step - loss: 2.2501\n",
      "Epoch 8/100\n",
      "91310/91310 [==============================] - 200s 2ms/step - loss: 2.1971\n",
      "Epoch 9/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 2.1490\n",
      "Epoch 10/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 2.1054\n",
      "Epoch 11/100\n",
      "91310/91310 [==============================] - 200s 2ms/step - loss: 2.0653\n",
      "Epoch 12/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 2.0269\n",
      "Epoch 13/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 1.9907\n",
      "Epoch 14/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.9555\n",
      "Epoch 15/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.9231\n",
      "Epoch 16/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.8908\n",
      "Epoch 17/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.8603\n",
      "Epoch 18/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 1.8267\n",
      "Epoch 19/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.7973\n",
      "Epoch 20/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.7648\n",
      "Epoch 21/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.7315\n",
      "Epoch 22/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.7008\n",
      "Epoch 23/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 1.6674\n",
      "Epoch 24/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.6334\n",
      "Epoch 25/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.5986\n",
      "Epoch 26/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.5623\n",
      "Epoch 27/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 1.5272\n",
      "Epoch 28/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 1.4924\n",
      "Epoch 29/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 1.4560\n",
      "Epoch 30/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 1.4173\n",
      "Epoch 31/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 1.3837\n",
      "Epoch 32/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.3513\n",
      "Epoch 33/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 1.3141\n",
      "Epoch 34/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.2793\n",
      "Epoch 35/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.2463\n",
      "Epoch 36/100\n",
      "91310/91310 [==============================] - 196s 2ms/step - loss: 1.2126\n",
      "Epoch 37/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.1785\n",
      "Epoch 38/100\n",
      "91310/91310 [==============================] - 196s 2ms/step - loss: 1.1462\n",
      "Epoch 39/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.1157\n",
      "Epoch 40/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.0853\n",
      "Epoch 41/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 1.0561\n",
      "Epoch 42/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 1.0268\n",
      "Epoch 43/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.9967\n",
      "Epoch 44/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.9744\n",
      "Epoch 45/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.9440\n",
      "Epoch 46/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.9207\n",
      "Epoch 47/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.8977\n",
      "Epoch 48/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.8679\n",
      "Epoch 49/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.8464\n",
      "Epoch 50/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.8232\n",
      "Epoch 51/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.8010\n",
      "Epoch 52/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.7803\n",
      "Epoch 53/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.7579\n",
      "Epoch 54/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.7441\n",
      "Epoch 55/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.7193\n",
      "Epoch 56/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.7025\n",
      "Epoch 57/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.6815\n",
      "Epoch 58/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.6645\n",
      "Epoch 59/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.6449\n",
      "Epoch 60/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.6318\n",
      "Epoch 61/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.6154\n",
      "Epoch 62/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.6049\n",
      "Epoch 63/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.5843\n",
      "Epoch 64/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.5724\n",
      "Epoch 65/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 0.5550\n",
      "Epoch 66/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.5494\n",
      "Epoch 67/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.5340\n",
      "Epoch 68/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.5241\n",
      "Epoch 69/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.5073\n",
      "Epoch 70/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.5011\n",
      "Epoch 71/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 0.4937\n",
      "Epoch 72/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.4763\n",
      "Epoch 73/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.4606\n",
      "Epoch 74/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.4624\n",
      "Epoch 75/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.4474\n",
      "Epoch 76/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.4387\n",
      "Epoch 77/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 0.4316\n",
      "Epoch 78/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.4151\n",
      "Epoch 79/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.4208\n",
      "Epoch 80/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.4076\n",
      "Epoch 81/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3972\n",
      "Epoch 82/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3958\n",
      "Epoch 83/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3794\n",
      "Epoch 84/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 0.3795\n",
      "Epoch 85/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.3727\n",
      "Epoch 86/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3685\n",
      "Epoch 87/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 0.3565\n",
      "Epoch 88/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.3571\n",
      "Epoch 89/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3505\n",
      "Epoch 90/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.3473\n",
      "Epoch 91/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3339\n",
      "Epoch 92/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3334\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3349\n",
      "Epoch 94/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3261\n",
      "Epoch 95/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.3203\n",
      "Epoch 96/100\n",
      "91310/91310 [==============================] - 199s 2ms/step - loss: 0.3221\n",
      "Epoch 97/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.3054\n",
      "Epoch 98/100\n",
      "91310/91310 [==============================] - 198s 2ms/step - loss: 0.3065\n",
      "Epoch 99/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.3084\n",
      "Epoch 100/100\n",
      "91310/91310 [==============================] - 197s 2ms/step - loss: 0.2909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12bdf24a8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_rnn.h5')\n",
    "\n",
    "# loaded_rnn = model_load('my_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, encoding, seed_text, num_chars, sequence_length, max_prob=False):\n",
    "\t'''\n",
    "\tThis function uses the trained model and encoding map to generate text\n",
    "\tof a sonnet for the requested number of characters, each time using the last \n",
    "\tsequence_length characters to predict the next character.\n",
    "\tInput:\n",
    "\t\tmodel: The trained model\n",
    "\t\tencoding: the mapping of characters to integers\n",
    "\t\tseed_text: the initial text used to predict the next characters\n",
    "\t\tnum_chars: the number of characters we want to predict\n",
    "\t\tsequence_length: length of the sequences trained on\n",
    "\t\tmax_prob: \n",
    "\t\t\tTrue = take the highest probability character\n",
    "\t\t\tFalse = take a character based on the probability distribution\n",
    "\t\n",
    "\tOuput:\n",
    "\t\tnew_text: text with num_chars characters of generated text in \n",
    "\t\t\t\t  addition to the seed text\n",
    "\t'''\n",
    "\t# Initialize with this seed text\n",
    "\tnew_text = seed_text\n",
    "\tfor _ in range(num_chars):\n",
    "\t\t# Convert current sequence from characters to integers\n",
    "\t\tsequence = [encoding[char] for char in new_text]\n",
    "\t\t# Only look at the newest 40 characters\n",
    "\t\tsequence = pad_sequences([sequence], maxlen=40, truncating='pre')\n",
    "\t\t# Apply one-hot encoding\n",
    "\t\tencoded_sequence = to_categorical(sequence, num_classes=len(encoding))\n",
    "\t\tif max_prob:\n",
    "\t\t\t# Predict character by max probability\n",
    "\t\t\tprob_index = model.predict_classes(encoded_sequence)\n",
    "\t\telse:\n",
    "\t\t\t# Predict character by probability distribution\n",
    "\t\t\tprob_indices = model.predict(encoded_sequence)[0]\n",
    "\t\t\tprob_index = np.random.choice(range(len(prob_indices)), p=prob_indices)\n",
    "\t\t# Convert integer back to character\n",
    "\t\tnew_char = ''\n",
    "\t\tfor char, index in encoding.items():\n",
    "\t\t\tif index == prob_index:\n",
    "\t\t\t\tnew_char = char\n",
    "\t\t\t\tbreak\n",
    "\t\tnew_text += new_char\n",
    "\n",
    "\treturn new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWordFrequency(input):\n",
    "    f = open(artist_file,'r')\n",
    "    words = [x for y in [l.split() for l in f.readlines()] for x in y]\n",
    "    data = sorted([(w, words.count(w)) for w in set(words)], key = lambda x:x[1], reverse=True)[:40] \n",
    "    most_words = [x[0] for x in data]\n",
    "    times_used = [int(x[1]) for x in data]\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(x=sorted(most_words), height=times_used, color = 'grey', edgecolor = 'black',  width=.5)\n",
    "    plt.xticks(rotation=45, fontsize=18)\n",
    "    plt.yticks(rotation=0, fontsize=18)\n",
    "    plt.xlabel('Most Common Words:', fontsize=18)\n",
    "    plt.ylabel('Number of Occurences:', fontsize=18)\n",
    "    plt.title('Most Commonly Used Words: %s' % (artist_file), fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "#  the keras model which is trained is defined as 'model' in this example\n",
    "model_json = model.to_json()\n",
    "\n",
    "\n",
    "with open(\"model_rnn.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_rnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "temp = 0.2 # this is essential\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model_rnn.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_rnn_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# loaded_model.save('model_rnn.hdf5')\n",
    "# loaded_model=load_model('model_rnn.hdf5') # this is complaining that 'temp' isn't initialized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will passing the model as 'model' work even if it can't load it?\n",
    "sonnet = generate_text(model, char_to_int, seed_text='from creatures', num_chars=60, sequence_length=40, max_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from creatures thour my fro be assth thour my gruth thour gruth thoull fro'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
