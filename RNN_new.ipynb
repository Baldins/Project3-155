{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout, Lambda\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.getcwd()+'/data/shakespeare.txt')\n",
    "text = f.readlines()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# chars = [x for x in chars if x not in newPun] # changed from newPunct\n",
    "\n",
    "# correct mappings\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "n_chars = len(chars)\n",
    "text_length = len(text)\n",
    "\n",
    "sentence_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "lstm_size = 200\n",
    "num_epochs = 100\n",
    "sequence_length = 40\n",
    "\n",
    "sentence_length = sequence_length\n",
    "skip = 1\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0,len(text)-sentence_length, skip):\n",
    "    seq_in = text[i:i + sentence_length]\n",
    "    seq_out = text[i + sentence_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "num_sentences = len(dataX)\n",
    "\n",
    "X = np.zeros((num_sentences, sentence_length, n_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, n_chars), dtype=np.bool)\n",
    "\n",
    "for i, sent in enumerate(dataX):\n",
    "    for j, n_c in enumerate(sent):\n",
    "        X[i, j, n_c] = 1\n",
    "    y[i, dataY[i]] = 1\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# X = np.array([to_categorical(seq, num_classes=n_chars) for seq in dataX])\n",
    "# # normalize\n",
    "# X = X / float(n_chars)\n",
    "# # one hot encode the output variable\n",
    "# y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm hard coding temp in the Lmbda line for ease of loading, temp = 0.2\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(sentence_length, n_chars)))\n",
    "model.add(Dense(n_chars, activation='softmax'))\n",
    "model.add(Lambda(lambda x: x / 1.2))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/francescabaldini/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "2575/2575 [==============================] - 70s 27ms/step - loss: 7.7105\n",
      "Epoch 2/100\n",
      "2575/2575 [==============================] - 67s 26ms/step - loss: 7.4656\n",
      "Epoch 3/100\n",
      "2575/2575 [==============================] - 63s 25ms/step - loss: 7.3077\n",
      "Epoch 4/100\n",
      "2575/2575 [==============================] - 58s 23ms/step - loss: 7.2411\n",
      "Epoch 5/100\n",
      "2575/2575 [==============================] - 70s 27ms/step - loss: 7.2139\n",
      "Epoch 6/100\n",
      "2575/2575 [==============================] - 67s 26ms/step - loss: 7.1945\n",
      "Epoch 7/100\n",
      "2575/2575 [==============================] - 74s 29ms/step - loss: 7.1762\n",
      "Epoch 8/100\n",
      "2575/2575 [==============================] - 62s 24ms/step - loss: 7.1526\n",
      "Epoch 9/100\n",
      "2575/2575 [==============================] - 57s 22ms/step - loss: 7.1203\n",
      "Epoch 10/100\n",
      "2575/2575 [==============================] - 59s 23ms/step - loss: 6.9864\n",
      "Epoch 11/100\n",
      "2575/2575 [==============================] - 58s 23ms/step - loss: 6.7927\n",
      "Epoch 12/100\n",
      "2575/2575 [==============================] - 60s 23ms/step - loss: 6.5507\n",
      "Epoch 13/100\n",
      "2575/2575 [==============================] - 68s 26ms/step - loss: 6.3130\n",
      "Epoch 14/100\n",
      "2575/2575 [==============================] - 74s 29ms/step - loss: 6.0940\n",
      "Epoch 15/100\n",
      "2575/2575 [==============================] - 89s 34ms/step - loss: 5.9071\n",
      "Epoch 16/100\n",
      "2575/2575 [==============================] - 94s 37ms/step - loss: 5.7512\n",
      "Epoch 17/100\n",
      "2575/2575 [==============================] - 80s 31ms/step - loss: 5.5726\n",
      "Epoch 18/100\n",
      "2575/2575 [==============================] - 76s 29ms/step - loss: 5.4108\n",
      "Epoch 19/100\n",
      "2575/2575 [==============================] - 67s 26ms/step - loss: 5.2196\n",
      "Epoch 20/100\n",
      "2575/2575 [==============================] - 71s 28ms/step - loss: 5.0830\n",
      "Epoch 21/100\n",
      "2575/2575 [==============================] - 77s 30ms/step - loss: 4.7752\n",
      "Epoch 22/100\n",
      "2575/2575 [==============================] - 70s 27ms/step - loss: 4.4753\n",
      "Epoch 23/100\n",
      "2575/2575 [==============================] - 70s 27ms/step - loss: 4.1765\n",
      "Epoch 24/100\n",
      "2575/2575 [==============================] - 68s 26ms/step - loss: 3.8635\n",
      "Epoch 25/100\n",
      "2575/2575 [==============================] - 82s 32ms/step - loss: 3.5186\n",
      "Epoch 26/100\n",
      "2575/2575 [==============================] - 90s 35ms/step - loss: 3.1868\n",
      "Epoch 27/100\n",
      "2575/2575 [==============================] - 82s 32ms/step - loss: 2.8873\n",
      "Epoch 28/100\n",
      "2575/2575 [==============================] - 84s 33ms/step - loss: 2.6388\n",
      "Epoch 29/100\n",
      "2575/2575 [==============================] - 89s 35ms/step - loss: 2.3073\n",
      "Epoch 30/100\n",
      "2575/2575 [==============================] - 71s 27ms/step - loss: 2.0427\n",
      "Epoch 31/100\n",
      "2575/2575 [==============================] - 59s 23ms/step - loss: 1.8155\n",
      "Epoch 32/100\n",
      "2575/2575 [==============================] - 53s 21ms/step - loss: 1.6036\n",
      "Epoch 33/100\n",
      "2575/2575 [==============================] - 54s 21ms/step - loss: 1.4778\n",
      "Epoch 34/100\n",
      "2575/2575 [==============================] - 56s 22ms/step - loss: 1.3046\n",
      "Epoch 35/100\n",
      "2575/2575 [==============================] - 58s 23ms/step - loss: 1.1803\n",
      "Epoch 36/100\n",
      "2575/2575 [==============================] - 64s 25ms/step - loss: 1.0483\n",
      "Epoch 37/100\n",
      "2575/2575 [==============================] - 53s 20ms/step - loss: 0.9728\n",
      "Epoch 38/100\n",
      "2575/2575 [==============================] - 46s 18ms/step - loss: 0.8662\n",
      "Epoch 39/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.7593\n",
      "Epoch 40/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.6890\n",
      "Epoch 41/100\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 0.6975\n",
      "Epoch 42/100\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 0.6814\n",
      "Epoch 43/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.6078\n",
      "Epoch 44/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.5344\n",
      "Epoch 45/100\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 0.5035\n",
      "Epoch 46/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.5056\n",
      "Epoch 47/100\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 0.4504\n",
      "Epoch 48/100\n",
      "2575/2575 [==============================] - 41s 16ms/step - loss: 0.4232\n",
      "Epoch 49/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.3881\n",
      "Epoch 50/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.3750\n",
      "Epoch 51/100\n",
      "2575/2575 [==============================] - 45s 18ms/step - loss: 0.3551\n",
      "Epoch 52/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.3740\n",
      "Epoch 53/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.3417\n",
      "Epoch 54/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.3154\n",
      "Epoch 55/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.2842\n",
      "Epoch 56/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.2676\n",
      "Epoch 57/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.2558\n",
      "Epoch 58/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.2509\n",
      "Epoch 59/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.2305\n",
      "Epoch 60/100\n",
      "2575/2575 [==============================] - 44s 17ms/step - loss: 0.2250\n",
      "Epoch 61/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.2169\n",
      "Epoch 62/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.2043\n",
      "Epoch 63/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.1888\n",
      "Epoch 64/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1768\n",
      "Epoch 65/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.1817\n",
      "Epoch 66/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.1757\n",
      "Epoch 67/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1701\n",
      "Epoch 68/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1600\n",
      "Epoch 69/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.1481\n",
      "Epoch 70/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1475\n",
      "Epoch 71/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1465\n",
      "Epoch 72/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.1430\n",
      "Epoch 73/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1407\n",
      "Epoch 74/100\n",
      "2575/2575 [==============================] - 42s 16ms/step - loss: 0.1496\n",
      "Epoch 75/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1695\n",
      "Epoch 76/100\n",
      "2575/2575 [==============================] - 43s 17ms/step - loss: 0.1833\n",
      "Epoch 77/100\n",
      "2575/2575 [==============================] - 46s 18ms/step - loss: 0.1903\n",
      "Epoch 78/100\n",
      "2176/2575 [========================>.....] - ETA: 6s - loss: 0.1739"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=num_epochs, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
