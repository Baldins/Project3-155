{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMM import unsupervised_HMM\n",
    "from HMM_helper import sample_sentence, parse_observations\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import os\n",
    "# you have to install pyphen\n",
    "# import pyphen\n",
    "from nltk.corpus import cmudict\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "\n",
    "\n",
    "#### 1) We're currently using their HMM file (now using pset6 solutions)\n",
    "#### 2) Rewrite the read_files function to include ours (eg. so all_words instead of poems) and keep the syllables and rhymes\n",
    "#### 3) LOAD OUR HMM MODEL - (don't need to anymore because of 1)\n",
    "#### 4) First sonnet: their HMM, using poems\n",
    "#### 5) Second sonnet: their HMM, using all_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.getcwd()+'/data/shakespeare.txt')\n",
    "\n",
    "all_words = []\n",
    "lines = f.readlines()\n",
    "new_lines = []\n",
    "sonnets=[]\n",
    "sonnet=[]\n",
    "poems = []\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    newPunct = \"!#$%&()*'+,./:;<=>?@[\\]^_`{|}~\"\n",
    "#     line = ''.join(c for c in line if c not in newPunct)\n",
    "    for punct in newPunct:\n",
    "        line = line.replace(punct, '')\n",
    "    if line.isdigit():\n",
    "        sonnets.append(sonnet)\n",
    "        sonnet = []\n",
    "    elif line.strip() == '':\n",
    "        pass\n",
    "    else:\n",
    "        all_words.append(nltk.word_tokenize(line)) # this is all tokens in each line\n",
    "        sonnet.append(line)\n",
    "sonnets.append(sonnet)\n",
    "del sonnets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punct(s):\n",
    "    '''\n",
    "    This function strips the punctuation of any given string. \n",
    "    Input: \n",
    "        s: string to strip punctuation from \n",
    "    Output: \n",
    "        string stripped of punctuation\n",
    "    '''\n",
    "    # newPunct = punctuation.replace(\"'\", \"\")\n",
    "    # newPunct = punctuation.replace(\"-\", \"\")\n",
    "    newPunct = '''!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~'''\n",
    "    return ''.join(c for c in s if c not in newPunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lower cell reads in the text file, then returns the list of words and the number of syllables in either the line or whole poem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(sep='poem'):\n",
    "    '''\n",
    "    This function reads the shakespeare and syllable files given to us.\n",
    "    Input: \n",
    "        sep: either 'line' or 'poem'. If line, shakeLines is a separate entry \n",
    "            per line, and if poem, shakeLines is a separate entry per poem\n",
    "    Output: \n",
    "        shakeLines: A 2D list with each element being a list of the words in the\n",
    "            line or poem\n",
    "        syllables: a dictionary with each key being a word and it's value \n",
    "            being how many syllables it has. The changed number of syllables\n",
    "            with the word being at the end of a line is currently ignored.\n",
    "    '''\n",
    "\n",
    "    # format: each line is an individual list of words in that line\n",
    "    shakeLines = []\n",
    "    # read in the shakespeare poems \n",
    "    if sep == 'line':\n",
    "        with open(\"./data/shakespeare.txt\") as poems:\n",
    "            for index, line in enumerate(poems):\n",
    "                # super jank way to get rid of line numbers, but it works!\n",
    "                if line != \"\\n\" and len(line) != 23 and len(line) != 22 and len(line) != 21:\n",
    "                    line = line.lower()\n",
    "                    shakeLines.append(re.findall(r\"[\\w']+\", strip_punct(line.rstrip(\"\\n\"))))\n",
    "    \n",
    "    # format: each poem is an individual list of words in that poem\n",
    "    if sep == 'poem':\n",
    "        file = open(\"./data/shakespeare.txt\")\n",
    "        data = file.read()\n",
    "        paragraph = data.split(\"\\n\\n\\n\")\n",
    "        for poem in paragraph:\n",
    "            poem = poem.replace('\\n', ' ')\n",
    "            poem = poem.lstrip()\n",
    "            poem = poem.split(' ', 1)[1]\n",
    "            poem = poem.lower()\n",
    "            shakeLines.append(re.findall(r\"[\\w']+\", strip_punct(poem.rstrip(\"\\n\"))))\n",
    "\n",
    "\n",
    "    # format: dictionary of how many syllables each word is\n",
    "    # note that syllable differences at end of lines are ignored\n",
    "    syllables = {}\n",
    "    with open(\"./data/Syllable_dictionary.txt\") as syllDict:\n",
    "        for line in syllDict:\n",
    "            split = line.split()\n",
    "            if len(split) == 3:\n",
    "                (key, end, val) = line.split()\n",
    "            else:\n",
    "                (key, val) = line.split()\n",
    "\n",
    "            # they're ordered by syllable length, so sometimes the E is last\n",
    "            try:\n",
    "                syllables[key] = int(val)\n",
    "            except:\n",
    "                syllables[key] = int(end)\n",
    "\n",
    "    rhymes = {}\n",
    "    if sep == 'line':\n",
    "        # print(shakeLines)\n",
    "        # sonnet format: abab cdcd efef gg\n",
    "        for j in range(len(shakeLines)-1):\n",
    "            line = shakeLines[j]\n",
    "            last_word = line[-1]\n",
    "            i = (j % 14) + 1\n",
    "            # abab\n",
    "            if i == 1 or i == 2 or i == 5 or i == 6 or i == 9 or i == 10:\n",
    "                rhymes = make_rhyming_dictionary(rhymes, last_word, shakeLines[j+2][-1])\n",
    "            # elif i == 3 or i == 4 or i == 7 or i == 8 or i == 11 or i == 12:\n",
    "                # rhymes = make_rhyming_dictionary(rhymes, last_word, shakeLines[j-2][-1])\n",
    "            elif i == 13:\n",
    "                rhymes = make_rhyming_dictionary(rhymes, last_word, shakeLines[j+1][-1])\n",
    "            # elif i == 14:\n",
    "            #     rhymes = make_rhyming_dictionary(rhymes, last_word, shakeLines[j-1][-1])\n",
    "\n",
    "    return shakeLines, syllables, rhymes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rhyming_dictionary(dictionary, word1, word2):\n",
    "    if word1 in dictionary:\n",
    "        lst = dictionary.get(word1)\n",
    "        if word2 not in lst:\n",
    "            new_lst = dictionary[word1]\n",
    "            new_lst.append(word2)\n",
    "            dictionary[word1] = new_lst\n",
    "    else:\n",
    "        dictionary[word1] = [word2]\n",
    "\n",
    "    if word2 in dictionary:\n",
    "        lst = dictionary.get(word2)\n",
    "        if word1 not in lst:\n",
    "            new_lst = dictionary[word2]\n",
    "            new_lst.append(word1)\n",
    "            dictionary[word2] = new_lst\n",
    "    else:\n",
    "        dictionary[word2] = [word1]\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakeLines, syllables, rhymes = read_files(sep='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function returns the sequences in the lines (not sure what this is to be honest), a dictionary with their encoding and the features (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(lines):\n",
    "    '''\n",
    "    This function returns the feature representation of a set of lines.\n",
    "    Input: \n",
    "        lines: An iterable object with each element being a list of strings\n",
    "    Output: \n",
    "        possiblePOS: the list of possible parts of speech, where the index of \n",
    "            each POS being the its number in the \n",
    "        POSlookup:  A 2D array being POS, [word, frequency] for the given POS\n",
    "        features: The feature representation of the input\n",
    "    '''\n",
    "    possiblePOS = []\n",
    "    POSlookup = []\n",
    "    features = []\n",
    "    for obs in lines:\n",
    "        # POS is a list of tuples being (word, POS)\n",
    "        POS = pos_tag(obs)\n",
    "        poemFeatures = []\n",
    "        # if it's a new POS, add it to the list\n",
    "        for pair in POS: \n",
    "            if pair[1] not in possiblePOS:\n",
    "                possiblePOS.append(pair[1])\n",
    "                POSlookup.append([])\n",
    "                POSlookup[possiblePOS.index(pair[1])].append([pair[0], 1])\n",
    "            else: \n",
    "                firstCol = [row[0] for row in POSlookup[possiblePOS.index(pair[1])]]\n",
    "                if pair[0] not in firstCol:\n",
    "                    POSlookup[possiblePOS.index(pair[1])].append([pair[0], 1])\n",
    "                else:\n",
    "                    index = firstCol.index(pair[0])\n",
    "                    POSlookup[possiblePOS.index(pair[1])][index][1] += 1\n",
    "            # we are simply indexing using the order in which they appear\n",
    "\n",
    "            poemFeatures.append(possiblePOS.index(pair[1]))\n",
    "            # print(POSlookup[possiblePOS.index(pair[1])])\n",
    "            \n",
    "\n",
    "\n",
    "        features.append(poemFeatures)\n",
    "    return possiblePOS, POSlookup, features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function below calls featurize. Syllables is provided by calling 'read_files' above\n",
    "\n",
    "example:\n",
    "poems, syllables, _ = read_files(sep='poem')\n",
    "lines, syllables, rhymes = read_files(sep='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(emission, POSlookup, syllables, reverse=False, lastWord=None):\n",
    "\t'''\n",
    "\tThis function generates a string given the emissions and the probabilities \n",
    "\tof a word being emitted given a certain\n",
    "\tInput:\n",
    "\t\temission: The list of emission, which represents the POS of the word\n",
    "\t\tPOSlookup: A 2D array being POS, [word, frequency] for the given POS\n",
    "\t\tsyllables: The dictionary of words and number of syllables each word has\n",
    "\t\treverse: Whether to start from beginning or end of line\n",
    "\t\trhymes: Dictionary of different rhymes\n",
    "\tOutput:\n",
    "\t\temStr: The sentence generated\n",
    "\t'''\n",
    "\tdone = False\n",
    "\tif reverse:\n",
    "\t\tassert(lastWord is not None)\n",
    "\t\twhile not done:\n",
    "\t\t\temStr = lastWord\n",
    "\t\t\ttry:\n",
    "\t\t\t\tsyllableCount = syllables[lastWord]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tsyllableCount = 2\n",
    "\t\t\t\tprint(lastWord)\n",
    "\t\t\tfor obs in emission:\n",
    "\t\t\t\temRate = [row[1] for row in POSlookup[obs]]\n",
    "\t\t\t\temWords = [row[0] for row in POSlookup[obs]]\n",
    "\t\t\t\temRate = np.array(emRate)\n",
    "\t\t\t\temRate = emRate/sum(emRate)\n",
    "\n",
    "\t\t\t\tindex = np.random.choice(np.arange(len(emRate)), p=emRate)\n",
    "\t\t\t\tnewWord = emWords[index]\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tsyllableCount += syllables[newWord]\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tsyllableCount += 2\n",
    "\t\t\t\t\tprint(newWord)\n",
    "\t\t\t\temStr = newWord + ' ' + emStr\n",
    "\t\t\t\tif syllableCount == 10:\n",
    "\t\t\t\t\tdone = True\n",
    "\t\t\t\t\tbreak\n",
    "\telse:\n",
    "\t\twhile not done:\n",
    "\t\t\temStr = ''\n",
    "\t\t\tsyllableCount = 0\n",
    "\t\t\tfor obs in emission: \n",
    "\t\t\t\temRate = [row[1] for row in POSlookup[obs]]\n",
    "\t\t\t\temWords = [row[0] for row in POSlookup[obs]]\n",
    "\t\t\t\temRate = np.array(emRate)\n",
    "\t\t\t\temRate = emRate/sum(emRate)\n",
    "\n",
    "\t\t\t\tindex = np.random.choice(np.arange(len(emRate)), p=emRate)\n",
    "\t\t\t\tnewWord = emWords[index]\n",
    "\t\t\t\tsyllableCount += syllables[newWord]\n",
    "\t\t\t\temStr = emStr + newWord + ' '\n",
    "\t\t\t\tif syllableCount == 10:\n",
    "\t\t\t\t\tdone = True\n",
    "\t\t\t\t\tbreak\n",
    "\treturn emStr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM = unsupervised_HMM(features, 25, 100)\n",
    "# emission, states = HMM.generate_emission(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonnet(poems, lines, syllables, rhymes=None):\n",
    "\tPOSList, POSlookup, features = featurize(poems)\n",
    "\tHMM = unsupervised_HMM(features, 25, 100)\n",
    "\temission, states = HMM.generate_emission(10)\n",
    "\tif rhymes is None:\n",
    "\t\tsonnet = \"\"\n",
    "\t\tfor i in range(14):\n",
    "\t\t\tline = generate_words(emission, POSlookup, syllables)\n",
    "\t\t\tsonnet = sonnet + line + \"\\n\"\n",
    "\n",
    "\telse:\n",
    "\t\t# abab cdcd efef gg\n",
    "\t\tsonnet = [\"\" for x in range(14)]\n",
    "\t\tline_idx = [0, 1, 4, 5, 8, 9, 12]\n",
    "\t\tfor i in line_idx:\n",
    "\t\t\t# choose a random word in the dictionary\n",
    "\t\t\tkey, val = random.choice(list(rhymes.items()))\n",
    "\t\t\t# choose a random word that rhymes with the previous one\n",
    "\t\t\tpair = np.random.choice(val)\n",
    "\t\t\tsonnet[i] += str(key)\n",
    "\t\t\tif i < 12:\n",
    "\t\t\t\tsonnet[i+2] += str(pair)\n",
    "\t\t\telse:\n",
    "\t\t\t\tsonnet[i+1] += str(pair)\n",
    "\t\tfor i in range(len(sonnet)):\n",
    "\t\t\tline = generate_words(emission, POSlookup, syllables, True, sonnet[i])\n",
    "\t\t\tsonnet[i] = line\n",
    "\t\tsonnet = \"\\n\".join(sonnet)\n",
    "\tprint(sonnet)\n",
    "\treturn sonnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems, syllables, _ = read_files(sep='poem')\n",
    "lines, syllables, rhymes = read_files(sep='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n",
      "childrens\n",
      "form if childrens another of seemed bind\n",
      "turn which self of towers all of dressed longer\n",
      "show of dignifies this in annexed kind\n",
      "that odour of looks this thou born stronger\n",
      "devise that needs the with gazed everywhere\n",
      "do which glass as eyes this of buried stand\n",
      "which guest upon summers all than i life\n",
      "mightst that face of i a after made land\n",
      "which purpose of lines this of been dearer\n",
      "which world in looks a of slandered contains\n",
      "that hate though numbers this though thy begin\n",
      "petty in parts the as famoused remains\n",
      "have worse barren as doth the since seen glad\n",
      "i that thee in bonds that of falsehood sad\n"
     ]
    }
   ],
   "source": [
    "sonnet2 = generate_sonnet(all_words, lines, syllables, rhymes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet1 = sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sonnet2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3207f325979b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msonnet2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sonnet2' is not defined"
     ]
    }
   ],
   "source": [
    "sonnet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the little love-god lying once asleep'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnet1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sonnet1.txt', 'w') as f:\n",
    "    f.write(str(sonnet1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
