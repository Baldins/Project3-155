{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from HMM import unsupervised_HMM\n",
    "from HMM_helper import sample_sentence, parse_observations\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first read in the sonnets and tokenize them. We will tokenize them both by line and by poem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.getcwd()+'/data/shakespeare.txt')\n",
    "\n",
    "all_words = []\n",
    "lines = f.readlines()\n",
    "new_lines = []\n",
    "sonnets=[]\n",
    "sonnet=[]\n",
    "poems = []\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    newPun = \"#$%&()*+/<=>@[\\]^_`{|}~1234567890-\"\n",
    "#     line = ''.join(c for c in line if c not in newPunct)\n",
    "    for punct in newPunct:\n",
    "        line = line.replace(punct, '')\n",
    "    if line.isdigit():\n",
    "        sonnets.append(sonnet)\n",
    "        sonnet = []\n",
    "    elif line.strip() == '':\n",
    "        pass\n",
    "    else:\n",
    "        sonnet.append(line)\n",
    "sonnets.append(sonnet)\n",
    "del sonnets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_tokens = [] # tokens per line\n",
    "poem_tokens = [] # tokens per poem\n",
    "single_poem_tokens = []\n",
    "for sonnet in sonnets:\n",
    "    poem_tokens.append(single_poem_tokens)\n",
    "    for line in sonnet:\n",
    "            line_tokens.append(nltk.word_tokenize(line)) # this is all tokens in each line\n",
    "            single_poem_tokens += nltk.word_tokenize(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will create a dictionary we will use later to ensure the sonnet rhyme scheme is adhered to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(dictionary, word1, word2):\n",
    "    \"\"\"\n",
    "    This function takes in a dictionary and two words. \n",
    "    It populates the dictionary with keys: word1\n",
    "    values: word2\n",
    "    \n",
    "    \"\"\"\n",
    "    if word1 in dictionary:\n",
    "        word_list = dictionary.get(word1)\n",
    "        if word2 not in word_list:\n",
    "            new_word_list = dictionary[word1]\n",
    "            new_word_list.append(word2)\n",
    "            dictionary[word1] = new_word_list\n",
    "    else:\n",
    "        dictionary[word1] = [word2]\n",
    "\n",
    "    if word2 in dictionary:\n",
    "        word_list = dictionary.get(word2)\n",
    "        if word1 not in word_list:\n",
    "            new_word_list = dictionary[word2]\n",
    "            new_word_list.append(word1)\n",
    "            dictionary[word2] = new_word_list\n",
    "    else:\n",
    "        dictionary[word2] = [word1]\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lower cell reads in the syllable dictionary, then returns the list of words and the number of syllables in either the line or whole poem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllable_dictionary is a dictionary with all the words in the sonnets\n",
    "# and how many syllables each word is \n",
    "syllables = {}\n",
    "with open(\"./data/Syllable_dictionary.txt\") as syllableDict:\n",
    "    for line in syllableDict:\n",
    "        split = line.split()\n",
    "        if len(split) == 3:\n",
    "            (key, end, val) = line.split()\n",
    "        else:\n",
    "            (key, val) = line.split()\n",
    "\n",
    "        # they're ordered by syllable length, so sometimes the E is last\n",
    "        try:\n",
    "            syllables[key] = int(val)\n",
    "        except:\n",
    "            syllables[key] = int(end)\n",
    "\n",
    "rhymes = {}\n",
    "# sonnet format: abab cdcd efef gg\n",
    "for j in range(len(line_tokens)-1):\n",
    "    line = line_tokens[j]\n",
    "    last_word = line[-1]\n",
    "    i = (j % 14) + 1\n",
    "    # using our dictionary function to create a dict where the values of a given key are all the words that rhyme with it\n",
    "    if i == 1 or i == 2 or i == 5 or i == 6 or i == 9 or i == 10:\n",
    "        rhymes = make_dictionary(rhymes, last_word, line_tokens[j+2][-1])\n",
    "    elif i == 13:\n",
    "        rhymes = make_dictionary(rhymes, last_word, line_tokens[j+1][-1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function returns the sequences in the lines (not sure what this is to be honest), a dictionary with their encoding and the features (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [] \n",
    "seq_encode = [] \n",
    "features = []\n",
    "\n",
    "# look through all tokens and \n",
    "for obs in line_tokens:\n",
    "    # assigning a POS tag to each token in that line\n",
    "    seq = pos_tag(obs)\n",
    "    poemFeatures = []\n",
    "    # if it's a new sequence, add it to the list\n",
    "    for pair in seq: \n",
    "        if pair[1] not in sequences:\n",
    "            sequences.append(pair[1])\n",
    "            seq_encode.append([])\n",
    "            seq_encode[sequences.index(pair[1])].append([pair[0], 1])\n",
    "        else: \n",
    "            firstCol = [row[0] for row in seq_encode[sequences.index(pair[1])]]\n",
    "            if pair[0] not in firstCol:\n",
    "                seq_encode[sequences.index(pair[1])].append([pair[0], 1])\n",
    "            else:\n",
    "                index = firstCol.index(pair[0])\n",
    "                seq_encode[sequences.index(pair[1])][index][1] += 1\n",
    "        \n",
    "        # poemFeatures now contains the features of each token in the line\n",
    "        poemFeatures.append(sequences.index(pair[1]))\n",
    "\n",
    "\n",
    "    features.append(poemFeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function below calls featurize. Syllables is provided by calling 'read_files' above\n",
    "\n",
    "example:\n",
    "poems, syllables, _ = read_files(sep='poem')\n",
    "lines, syllables, rhymes = read_files(sep='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(emission, POSlookup, syllables, reverse=False, lastWord=None):\n",
    "    '''\n",
    "    This function generates a string given the emissions and the probabilities \n",
    "    of a word being emitted given a certain\n",
    "    Input:\n",
    "        emission: The list of emission, which represents the POS of the word\n",
    "        POSlookup: A 2D array being POS, [word, frequency] for the given POS\n",
    "        syllables: The dictionary of words and number of syllables each word has\n",
    "        reverse: Whether to start from beginning or end of line\n",
    "        rhymes: Dictionary of different rhymes\n",
    "    Output:\n",
    "        emStr: The sentence generated\n",
    "    '''\n",
    "    done = False\n",
    "    if reverse:\n",
    "        assert(lastWord is not None)\n",
    "        while not done:\n",
    "            emStr = lastWord\n",
    "            try:\n",
    "                syllableCount = syllables[lastWord]\n",
    "            except:\n",
    "                syllableCount = 2\n",
    "                print(lastWord)\n",
    "            for obs in emission:\n",
    "                emRate = [row[1] for row in POSlookup[obs]]\n",
    "                emWords = [row[0] for row in POSlookup[obs]]\n",
    "                emRate = np.array(emRate)\n",
    "                emRate = emRate/sum(emRate)\n",
    "\n",
    "                index = np.random.choice(np.arange(len(emRate)), p=emRate)\n",
    "                newWord = emWords[index]\n",
    "                try:\n",
    "                    syllableCount += syllables[newWord]\n",
    "                except:\n",
    "                    syllableCount += 2\n",
    "                    print(newWord)\n",
    "                emStr = newWord + ' ' + emStr\n",
    "                if syllableCount == 10:\n",
    "                    done = True\n",
    "                    break\n",
    "    else:\n",
    "        while not done:\n",
    "            emStr = ''\n",
    "            syllableCount = 0\n",
    "            for obs in emission: \n",
    "                emRate = [row[1] for row in POSlookup[obs]]\n",
    "                emWords = [row[0] for row in POSlookup[obs]]\n",
    "                emRate = np.array(emRate)\n",
    "                emRate = emRate/sum(emRate)\n",
    "\n",
    "                index = np.random.choice(np.arange(len(emRate)), p=emRate)\n",
    "                newWord = emWords[index]\n",
    "                syllableCount += syllables[newWord]\n",
    "                emStr = emStr + newWord + ' '\n",
    "                if syllableCount == 10:\n",
    "                    done = True\n",
    "                    break\n",
    "    return emStr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-023dc11367c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mHMM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munsupervised_HMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0memission\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHMM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_emission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_states' is not defined"
     ]
    }
   ],
   "source": [
    "# Choose our model parameters\n",
    "num_hidden_states = 25\n",
    "iterations = 100\n",
    "\n",
    "HMM = unsupervised_HMM(features, num_hidden_states, iterations)\n",
    "emission, states = HMM.generate_emission(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets generate our sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing before dictionary was set up properly\n",
    "# if rhymes is None:\n",
    "#     sonnet = \"\"\n",
    "#     for i in range(14):\n",
    "#         line = generate_words(emission, POSlookup, syllables)\n",
    "#         sonnet = sonnet + line + \"\\n\"\n",
    "\n",
    "# else:\n",
    "    \n",
    "    \n",
    "# abab cdcd efef gg\n",
    "sonnet = [\"\" for x in range(14)]\n",
    "line_idx = [0, 1, 4, 5, 8, 9, 12]\n",
    "for i in line_idx:\n",
    "    # choose a random word in the dictionary\n",
    "    key, val = random.choice(list(rhymes.items()))\n",
    "    # choose a random word that rhymes with the previous one\n",
    "    pair = np.random.choice(val)\n",
    "    sonnet[i] += str(key)\n",
    "    if i < 12:\n",
    "        sonnet[i+2] += str(pair)\n",
    "    else:\n",
    "        sonnet[i+1] += str(pair)\n",
    "for i in range(len(sonnet)):\n",
    "    line = generate_words(emission, seq_encode, syllables, True, sonnet[i])\n",
    "    sonnet[i] = line\n",
    "sonnet = \"\\n\".join(sonnet)\n",
    "with open('sonnet1.txt', 'w') as f:\n",
    "    f.write(str(sonnet))\n",
    "print(sonnet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
