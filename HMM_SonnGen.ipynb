{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from HMM import unsupervised_HMM\n",
    "from HMM_helper import sample_sentence, parse_observations\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first read in the sonnets and tokenize them. We will tokenize them both by line and by poem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.getcwd()+'/data/shakespeare.txt')\n",
    "lines = f.readlines()\n",
    "all_words = []\n",
    "new_lines = []\n",
    "sonnets=[]\n",
    "sonnet=[]\n",
    "poems = []\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    newPun = \"!?.;:'#$%&(,)*+/<=>@][^_`{|}~-\"\n",
    "#     line = ''.join(c for c in line if c not in newPunct)\n",
    "    for punct in newPun:\n",
    "        line = line.replace(punct, '')\n",
    "    if line.isdigit():\n",
    "        sonnets.append(sonnet)\n",
    "        sonnet = []\n",
    "    elif line.strip() == '':\n",
    "        pass\n",
    "    else:\n",
    "        sonnet.append(line)\n",
    "sonnets.append(sonnet)\n",
    "del sonnets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_tokens = [] # tokens per line\n",
    "poem_tokens = [] # tokens per poem\n",
    "single_poem_tokens = []\n",
    "words = []\n",
    "\n",
    "for sonnet in sonnets:\n",
    "    poem_tokens.append(single_poem_tokens)\n",
    "    single_poem_tokens = []\n",
    "    for line in sonnet:\n",
    "        line_tokens.append(nltk.word_tokenize(line)) # this is all tokens in each line\n",
    "        single_poem_tokens += nltk.word_tokenize(line)\n",
    "        words += nltk.word_tokenize(line)\n",
    "del(poem_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17371"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will create a dictionary we will use later to ensure the sonnet rhyme scheme is adhered to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(dictionary, word1, word2):\n",
    "    \"\"\"\n",
    "    This function takes in a dictionary and two words. \n",
    "    It populates the dictionary with keys: word1\n",
    "    values: word2\n",
    "    \n",
    "    \"\"\"\n",
    "    if word1 in dictionary:\n",
    "        word_list = dictionary.get(word1)\n",
    "        if word2 not in word_list:\n",
    "            new_word_list = dictionary[word1]\n",
    "            new_word_list.append(word2)\n",
    "            dictionary[word1] = new_word_list\n",
    "    else:\n",
    "        dictionary[word1] = [word2]\n",
    "\n",
    "    if word2 in dictionary:\n",
    "        word_list = dictionary.get(word2)\n",
    "        if word1 not in word_list:\n",
    "            new_word_list = dictionary[word2]\n",
    "            new_word_list.append(word1)\n",
    "            dictionary[word2] = new_word_list\n",
    "    else:\n",
    "        dictionary[word2] = [word1]\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lower cell has two functions 1) It reads in the syllable dictionary, then returns the list of words and the number of syllables in either the line or whole poem. 2) It creates a rhyming dictionary with the keys being words found in the sonnets and the values being words that rhyme with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllable_dictionary is a dictionary with all the words in the sonnets\n",
    "# and how many syllables each word is \n",
    "syllables = {}\n",
    "with open(\"./data/Syllable_dictionary.txt\") as syllableDict:\n",
    "    for line in syllableDict:\n",
    "        split = line.split()\n",
    "        if len(split) == 3:\n",
    "            (key, end, val) = line.split()\n",
    "        else:\n",
    "            (key, val) = line.split()\n",
    "\n",
    "        # they're ordered by syllable length, so sometimes the E is last\n",
    "        try:\n",
    "            syllables[key] = int(val)\n",
    "        except:\n",
    "            syllables[key] = int(end)\n",
    "\n",
    "rhymes = {}\n",
    "# sonnet format: abab cdcd efef gg\n",
    "for j in range(len(line_tokens)-1):\n",
    "    line = line_tokens[j]\n",
    "    last_word = line[-1]\n",
    "    i = (j % 14) + 1\n",
    "    # using our dictionary function to create a dict where the values of a given key are all the words that rhyme with it\n",
    "    if i == 1 or i == 2 or i == 5 or i == 6 or i == 9 or i == 10:\n",
    "        rhymes = make_dictionary(rhymes, last_word, line_tokens[j+2][-1])\n",
    "    elif i == 13:\n",
    "        rhymes = make_dictionary(rhymes, last_word, line_tokens[j+1][-1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell returns the words in the poems as sequences, with a POS tag to each one and finally a list of features in each sonnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [] \n",
    "seq_encode = [] \n",
    "features = []\n",
    "\n",
    "# look through all tokens and \n",
    "for obs in poem_tokens:\n",
    "    # assigning a POS tag to each token in that line\n",
    "    seq = pos_tag(obs)\n",
    "    poemFeatures = []\n",
    "    # if it's a new sequence, add it to the list\n",
    "    for pair in seq: \n",
    "        if pair[1] not in sequences:\n",
    "            sequences.append(pair[1])\n",
    "            seq_encode.append([])\n",
    "            seq_encode[sequences.index(pair[1])].append([pair[0], 1])\n",
    "        else: \n",
    "            firstCol = [row[0] for row in seq_encode[sequences.index(pair[1])]]\n",
    "            if pair[0] not in firstCol:\n",
    "                seq_encode[sequences.index(pair[1])].append([pair[0], 1])\n",
    "            else:\n",
    "                index = firstCol.index(pair[0])\n",
    "                seq_encode[sequences.index(pair[1])][index][1] += 1\n",
    "        \n",
    "        # poemFeatures now contains the features of each token in the line\n",
    "        poemFeatures.append(sequences.index(pair[1]))\n",
    "\n",
    "\n",
    "    features.append(poemFeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a word given a specific emission probability and the sequence encodings from earlier. It will be used in the generation of the sonnet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(emission, seq_encode, syllables, reverse=False, lastWord=None):\n",
    "    '''\n",
    "    This function generates a string given the emissions and the probabilities \n",
    "    of a word being emitted given a certain\n",
    "    \n",
    "    '''\n",
    "    done = False\n",
    "    if reverse:\n",
    "        assert(lastWord is not None)\n",
    "        while not done:\n",
    "            emStr = lastWord\n",
    "            try:\n",
    "                syllableCount = syllables[lastWord]\n",
    "            except:\n",
    "                syllableCount = 2\n",
    "                print(lastWord)\n",
    "            for obs in emission:\n",
    "                emRate = [row[1] for row in seq_encode[obs]]\n",
    "                emWords = [row[0] for row in seq_encode[obs]]\n",
    "                emRate = np.array(emRate)\n",
    "                emRate = emRate/sum(emRate)\n",
    "\n",
    "                index = np.random.choice(np.arange(len(emRate)), p=emRate)\n",
    "                newWord = emWords[index]\n",
    "                try:\n",
    "                    syllableCount += syllables[newWord]\n",
    "                except:\n",
    "                    syllableCount += 2\n",
    "                    print(newWord)\n",
    "                emStr = newWord + ' ' + emStr\n",
    "                if syllableCount == 10:\n",
    "                    done = True\n",
    "                    break\n",
    "    else:\n",
    "        while not done:\n",
    "            emStr = ''\n",
    "            syllableCount = 0\n",
    "            for obs in emission: \n",
    "                emRate = [row[1] for row in seq_encode[obs]]\n",
    "                emWords = [row[0] for row in seq_encode[obs]]\n",
    "                emRate = np.array(emRate)\n",
    "                emRate = emRate/sum(emRate)\n",
    "\n",
    "                index = np.random.choice(np.arange(len(emRate)), p=emRate)\n",
    "                newWord = emWords[index]\n",
    "                syllableCount += syllables[newWord]\n",
    "                emStr = emStr + newWord + ' '\n",
    "                if syllableCount == 10:\n",
    "                    done = True\n",
    "                    break\n",
    "    return emStr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n"
     ]
    }
   ],
   "source": [
    "# Choose our model parameters\n",
    "num_hidden_states = 25\n",
    "iterations = 100\n",
    "\n",
    "# train our model on the features\n",
    "HMM = unsupervised_HMM(features, num_hidden_states, iterations)\n",
    "\n",
    "# generate emission probabilities and states\n",
    "emission, states = HMM.generate_emission(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets generate our sonnet\n",
    "\n",
    "Note that the output of this was submitted on Piazza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abab cdcd efef gg\n",
    "sonnet = [\"\" for x in range(14)]\n",
    "line_idx = [0, 1, 4, 5, 8, 9, 12]\n",
    "for i in line_idx:\n",
    "    # choose a random word in the dictionary\n",
    "    key, val = random.choice(list(rhymes.items()))\n",
    "    # choose a random word that rhymes with the previous one\n",
    "    pair = np.random.choice(val)\n",
    "    sonnet[i] += str(key)\n",
    "    if i < 12:\n",
    "        sonnet[i+2] += str(pair)\n",
    "    else:\n",
    "        sonnet[i+1] += str(pair)\n",
    "for i in range(len(sonnet)):\n",
    "    line = generate_words(emission, seq_encode, syllables, True, sonnet[i])\n",
    "    sonnet[i] = line\n",
    "sonnet = \"\\n\".join(sonnet)\n",
    "with open('sonnet1.txt', 'w') as f:\n",
    "    f.write(str(sonnet))\n",
    "print(sonnet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array(HMM.A)\n",
    "O = np.array(HMM.O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "plt.imshow(A, vmax=1.0)\n",
    "plt.colorbar()\n",
    "plt.title('Sparsity of A matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(O)[:, :O.shape[1]], vmax=0.1, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Sparsity of O matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "newPunct = \"!#$%&()*+,./:';<=>?@[\\]^_`{|}~\"\n",
    "words = [x for x in words if x not in newPunct]\n",
    "data = sorted([(w, words.count(w)) for w in set(words)], key = lambda x:x[1], reverse=True)[:40] \n",
    "most_words = [x[0] for x in data]\n",
    "most_words = sorted(most_words)\n",
    "del(most_words[0]) # this was an 's'\n",
    "times_used = [int(x[1]) for x in data]\n",
    "del(times_used[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the word frequencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(most_words, height=times_used, color = 'grey', edgecolor = 'black',  width=.5)\n",
    "plt.xticks(rotation=45, fontsize=18)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xlabel('Most Common Words:', fontsize=18)\n",
    "plt.ylabel('Number of Occurences:', fontsize=18)\n",
    "plt.title('Most Commonly Used Words: %s' % ('data/shakespeare.txt'), fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
